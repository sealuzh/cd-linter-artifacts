image: tgalopin/maven-javafx

stages:
  - build
  - test
  - report
  - deploy

maven_jar:
  stage: build
  script:
    - echo "[+] Building Project Jar With Maven"
    - cd Notes
    - mvn jfx:jar -B -Dmaven.test.skip=true
    - echo "[-] Building Jar Done"

maven_native:
  stage: build
  script:
    - echo "[+] Building Project Native With Maven"
    - cd Notes
    - mvn jfx:native -B -Dmaven.test.skip=true
    - echo "[-] Building Native Done"
  only:
    - release


maven_test:
  stage: test
  script:
    - echo "[+] Testing Project"
    - cd Notes
    - mvn test -B
    - echo "[-] Finished Testing"
  artifacts:
    paths:
      - Notes/target/surefire-reports/


reports:
  image: node:4.2.2
  stage: report
  dependencies:
    - maven_test
  script:
    - cd ./scripts
    - npm install
    - ./loc.sh
    - ./test.sh
  artifacts:
    paths:
      - scripts/loc/
      - scripts/test/
  only:
    - master


pages:
  stage: deploy
  dependencies:
    - reports
  script:
    - mkdir -p public/loc/
    - mv scripts/loc/ public/
    - mv scripts/test/ public/
  artifacts:
    paths:
      - public
    expire_in: 14 days
  only:
    - master


Wiki2PDF:
  # version : 1.2.0
  # License : MIT
  #  Author : Andrea Ligios

  variables:
    SCRIPT_VERSION: "1.2.0"
    
    # Details of your self-hosted Gitlab's installation? 
    # Considering "https://foo.bar/" GITLAB_DOMAIN is "foo.bar".
    #GITLAB_DOMAIN: "foo.bar"
    GITLAB_DOMAIN: "gitlab.com"
    
    # Considering "https://foo.bar/" GITLAB_SCHEME is "https".
    #GITLAB_SCHEME: "http"
    GITLAB_SCHEME: "https"
    

  only:
    # Wiki2PDF will run only when triggered by a Webhook...:
    - triggers
    # ... or after pressing the "Run Pipeline" button on Pipelines page:
    - web

  artifacts:
    # These are the files you then download after the job has finished.
    untracked: true

  script:
    # It's necessary to clone the WIKI which is a separate repository.
    - git clone http://gitlab-ci-token:${CI_BUILD_TOKEN}@${GITLAB_DOMAIN}/${CI_PROJECT_NAMESPACE}/${CI_PROJECT_NAME}.wiki.git
    # We'll also need to download the attached resources (eg. images), which are stored in an unaccessible /uploads folder,
    # so that during the PDF conversion, the relatively /uploads files referenced by the .md files will be accessible.
    
    - mkdir tmp
    - mkdir uploads
    - mkdir http

    ########################################
    #             WIKI IMAGES              #
    ########################################
    - |
      if [ "$(grep -oh '!\[.*](/uploads/[^\)]*' ${CI_PROJECT_NAME}.wiki/*.md)" == "" ]; then 
        echo "No Wiki resources to download"
      else
        # Matching only markdown images "![]()" (not markdown URLs like "[]()")
        grep -oh '!\[.*](/uploads/[^\)]*' ${CI_PROJECT_NAME}.wiki/*.md > tmp/tmp-wiki-1.txt
        
        # Printing all the Wiki images crawled in all the files.
        # Using printf because "echo -e" is inconsistent between different echo implementations
        printf "\n-------------------------------------\nDETECTING WIKI IMAGES:\n-------------------------------------\n"
        echo | cat tmp/tmp-wiki-1.txt
        # Using -- is necessary to explicit that options are over (since the row is starting with "--... and gets interpreted as an unrecognized option)
        printf -- "-------------------------------------\n\n"

        # Removing the textual part so that subsequent greps won't get false positive with "uploads/" in textual part, eg. [foo uploads/bar](/uploads/...)
        grep -oh '\](/uploads/.*' tmp/tmp-wiki-1.txt > tmp/tmp-wiki-2.txt
        # Extracting the files to download
        grep -oh 'uploads\/[^\)]*' tmp/tmp-wiki-2.txt > tmp/wiki-resources-files-to-download.txt
        # Extracting the folders to create
        grep -oh 'upload.*[\\\/]' tmp/wiki-resources-files-to-download.txt > tmp/wiki-resources-folders-to-create.txt

        # Creating /uploads subfolders
        printf "\n-------------------------------------\nCREATING FOLDERS STRUCTURE:\n-------------------------------------\n"
        while IFS='' read -r line || [[ -n "$line" ]]; do
          if [ ! -d ${line} ]; then
            mkdir -p ${line}
            printf "NEW FOLDER ========> Folder ${line} has been created\n"
          else
            printf "DUPLICATE FOLDER ==> Folder ${line} exists, safely skipping it...\n"
          fi
        done < "tmp/wiki-resources-folders-to-create.txt"
        printf -- "-------------------------------------\n\n"

        # Filling those subfolders with the resources
        printf "\n-------------------------------------\nDOWNLOADING WIKI IMAGES:\n-------------------------------------\n"
        while IFS='' read -r line || [[ -n "$line" ]]; do
          if [ ! -f ${line} ]; then
            wget -O ${line} ${GITLAB_SCHEME}://${GITLAB_DOMAIN}/${CI_PROJECT_NAMESPACE}/${CI_PROJECT_NAME}/${line}
          else
            printf "DUPLICATE FILE ====> File ${line} exists, safely skipping it...\n\n"
          fi 
        done < "tmp/wiki-resources-files-to-download.txt"
      fi


    ########################################
    #          HTTP/HTTPS IMAGES           #
    ########################################
    - |
      if [ "$(grep -oh '!\[.*](http[s]\?://[^\)]*' ${CI_PROJECT_NAME}.wiki/*.md)" == "" ]; then 
        echo "No HTTP/HTTPS resources to download"
      else

        # Matching only markdown images "![]()" (not markdown URLs like "[]()")
        grep -oh '!\[.*](http[s]\?://[^\)]*' ${CI_PROJECT_NAME}.wiki/*.md > tmp/tmp-http-1.txt
        
        # Printing all the HTTP/HTTPS images crawled in all the files.
        printf "\n-------------------------------------\nDETECTING HTTP/HTTPS IMAGES:\n-------------------------------------\n"
        echo | cat tmp/tmp-http-1.txt
        printf -- "-------------------------------------\n\n"

        # Removing the textual part so that subsequent greps won't get false positive with "uploads/" in textual part, eg. [foo http://bar](http://foobar/...)
        grep -oh '\](http[s]\?://.*' tmp/tmp-http-1.txt > tmp/tmp-http-2.txt
        
        # Extracting the files to download
        grep -oh 'http[s]\?://.*[^\)]*' tmp/tmp-http-2.txt > tmp/http-resources-files-to-download.txt
        
        # Extracting the folders to create
        grep -oh 'http[s]\?://.*[\\\/]' tmp/http-resources-files-to-download.txt > tmp/http-resources-folders-to-create.txt
        
        # Renaming all the URLS except the downloading ones into real folders paths
        sed -i 's/http:\//http/g'  ${CI_PROJECT_NAME}.wiki/*.md
        sed -i 's/http:\//http/g'  tmp/http-resources-folders-to-create.txt
        sed -i 's/https:\//http/g' ${CI_PROJECT_NAME}.wiki/*.md       
        sed -i 's/https:\//http/g' tmp/http-resources-folders-to-create.txt
      
        # Creating /http subfolders
        printf "\n-------------------------------------\nCREATING FOLDERS STRUCTURE:\n-------------------------------------\n"
        while IFS='' read -r line || [[ -n "$line" ]]; do
          if [ ! -d ${line} ]; then
            mkdir -p ${line}
            printf "NEW FOLDER ========> Folder ${line} has been created\n"
          else
            printf "DUPLICATE FOLDER ==> Folder ${line} exists, safely skipping it...\n"
          fi
        done < "tmp/http-resources-folders-to-create.txt"
        printf -- "-------------------------------------\n\n"

        # Filling those subfolders with the resources
        printf "\n-------------------------------------\nDOWNLOADING HTTP/HTTPS IMAGES:\n-------------------------------------\n"
        while IFS='' read -r line || [[ -n "$line" ]]; do
          echo ${line}
          declare localPath
          localPath=$(sed 's/http[s]\?:\//http/' <<< ${line})
          if [ ! -f ${localPath} ]; then
            wget -O ${localPath} ${line}
          else
            printf "DUPLICATE FILE ====> File ${localPath} exists, safely skipping it...\n\n"
          fi 
        done < "tmp/http-resources-files-to-download.txt"
        printf -- "-------------------------------------\n\n"
      fi

    # Let's create and fill wikis folder so it'll appear as an untracked (not yet committed) folder, 
    # and move the files there so that they'll be available as artifacts to download
    - mkdir -p wikis/source
    - cp ${CI_PROJECT_NAME}.wiki/*.md wikis/source
    - rm -rf ${CI_PROJECT_NAME}.wiki

    # Time to rock. Let's start MarkDown to PDF conversion
    - find ./wikis/source -name '*.md' -exec sh -c 'kramdown $0 --output pdf > $0.pdf' {} \;

    # cleanup
    - mv wikis/source/*.pdf wikis
    - mv uploads wikis/source
    - mv http wikis/source
    - mv tmp wikis/source
    
  before_script:
    - | 
      printf -- "\n\n\n"
      printf -- " ___________________________________________________\n"
      printf -- "/                                                   \\ \n"
      printf -- "|                  Wiki2PDF v${SCRIPT_VERSION}                  |\n"
      printf -- "|                                                   |\n"
      printf -- "#===================================================#\n"
      printf -- "|                                                   |\n"
      printf -- "|         Turns GitLab's Wiki pages into PDF        |\n"
      printf -- "|                                                   |\n"
      printf -- "|                                                   |\n"
      printf -- "|                                                   |\n"
      printf -- "|             Created by Andrea Ligios              |\n"
      printf -- "|            Released under MIT License             |\n"
      printf -- "|                                                   |\n"
      printf -- "|    https://gitlab.com/Andrea.Ligios/Wiki2PDF/     |\n"
      printf -- "\\___________________________________________________/\n\n\n\n"

    - apt-get update -y
    - apt-get install -y rubygems
    - gem install kramdown
    - gem install prawn
    - gem install prawn-table

