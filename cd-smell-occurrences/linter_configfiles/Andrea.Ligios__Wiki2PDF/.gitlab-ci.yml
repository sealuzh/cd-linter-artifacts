Wiki2PDF: 
  # version : 2.0.0
  # License : MIT
  #  Author : Andrea Ligios

  image: registry.gitlab.com/andrea.ligios/wiki2pdf

  variables:
    SCRIPT_VERSION: "2.0.0"
    
    # Details of your self-hosted Gitlab's installation? 
    # Considering "https://foo.bar/" GITLAB_DOMAIN is "foo.bar".
    #GITLAB_DOMAIN: "foo.bar"
    GITLAB_DOMAIN: "gitlab.com"
    
    # Considering "https://foo.bar/" GITLAB_SCHEME is "https".
    #GITLAB_SCHEME: "http"
    GITLAB_SCHEME: "https"    
    
    # Subfolders separator, used to separate path entries in generated file names, eg /foo/bar/baz.md -> ./foo@bar@baz.pdf
    SEPARATOR: "@"
    #SEPARATOR: "-"
    #SEPARATOR: "_"

  only:
    # Wiki2PDF will run only when triggered by a Webhook...:
    - triggers
    # ... or after pressing the "Run Pipeline" button on Pipelines page:
    - web

  artifacts:
    # These are the files you then download after the job has finished.
    untracked: true

  script:
    # It's necessary to clone the WIKI which is a separate repository.
    - git clone ${GITLAB_SCHEME}://gitlab-ci-token:${CI_BUILD_TOKEN}@${GITLAB_DOMAIN}/${CI_PROJECT_NAMESPACE}/${CI_PROJECT_NAME}.wiki.git
    # We'll also need to download the attached resources (eg. images), which are stored in an unaccessible /uploads folder,
    # so that during the PDF conversion, the relatively /uploads files referenced by the .md files will be accessible.

    - mkdir -p wiki/source/uploads
    - cd ${CI_PROJECT_NAME}.wiki
    - mkdir tmp
    - mkdir http
    
    ########################################
    #     WIKI IMAGES - MARKDOWN FILES     #
    ########################################
    - |
      if [ "$(grep -Roh --include '*.md' '!\[.*](/uploads/[^\)]*' .)" == "" ]; then 
         echo "No Wiki resources to download"
      else
        # Matching only markdown images "![]()" (not markdown URLs like "[]()")
        #grep -Roh --include '*.md' '!\[.*](/uploads/[^\)]*' . > tmp/tmp-wiki-md-1.txt
        grep -PRoh --include '*.md' '\!\[.*?\]\(/uploads\/[^\)]*' . > tmp/tmp-wiki-md-1.txt
        
        # Printing all the Wiki images crawled in all the files.
        # Using printf because "echo -e" is inconsistent between different echo implementations
        printf "\n-------------------------------------\nDETECTING WIKI IMAGES:\n-------------------------------------\n"
        cat tmp/tmp-wiki-md-1.txt
        # Using -- is necessary to explicit that options are over (since the row is starting with "--... and gets interpreted as an unrecognized option)
        printf -- "-------------------------------------\n\n"

        # Removing the textual part so that subsequent greps won't get false positive with "uploads/" in textual part, eg. [foo uploads/bar](/uploads/...)
        grep -oh '\](/uploads/.*' tmp/tmp-wiki-md-1.txt > tmp/tmp-wiki-md-2.txt
        # Extracting the files to download
        grep -oh 'uploads\/[^\)]*' tmp/tmp-wiki-md-2.txt > tmp/wiki-resources-md-files-to-download.txt
        # Extracting the folders to create
        grep -oh 'upload.*[\\\/]' tmp/wiki-resources-md-files-to-download.txt > tmp/wiki-resources-md-folders-to-create.txt

        # Renaming all the base ![*](/uploads/*) into relative to currentp path ![*](uploads/*)     
        find . -name '*.md' -type f -print0 | xargs -0 sed -E -i 's,(!\[.*])\(/uploads/,\1(uploads/,g'
        # Thanks to Wiktor StribiÅ¼ew for the help with the Regex (https://stackoverflow.com/q/52662006/1654265)
        
        # Creating /uploads subfolders
        printf "\n-------------------------------------\nCREATING FOLDERS STRUCTURE:\n-------------------------------------\n"
        while IFS='' read -r line || [[ -n "$line" ]]; do
          if [ ! -d ${line} ]; then
            mkdir -p ../wiki/source/${line}
            printf "NEW FOLDER ========> Folder ${line} has been created\n"
          else
            printf "DUPLICATE FOLDER ==> Folder ${line} exists, safely skipping it...\n"
          fi
        done < "tmp/wiki-resources-md-folders-to-create.txt"
        printf -- "-------------------------------------\n\n"

        # Filling those subfolders with the resources
        printf "\n-------------------------------------\nDOWNLOADING WIKI IMAGES:\n-------------------------------------\n"
        while IFS='' read -r line || [[ -n "$line" ]]; do
          if [ ! -f ${line} ]; then
            wget -O ../wiki/source/${line} ${GITLAB_SCHEME}://${GITLAB_DOMAIN}/${CI_PROJECT_NAMESPACE}/${CI_PROJECT_NAME}/${line}
          else
            printf "DUPLICATE FILE ====> File ${line} exists, safely skipping it...\n\n"
          fi 
        done < "tmp/wiki-resources-md-files-to-download.txt"
      fi


    ########################################
    #  HTTP/HTTPS IMAGES - MARKDOWN FILES  #
    ########################################
    - |
      if [ "$(grep -Roh --include '*.md' '!\[.*](http[s]\?://[^\)]*' .)" == "" ]; then 
        echo "No HTTP/HTTPS resources to download"
      else

        # Matching only markdown images "![]()" (not markdown URLs like "[]()")
        #grep -Roh --include '*.md' '!\[.*](http[s]\?://[^\)]*' . > tmp/tmp-http-md-1.txt
        grep -PRoh --include '*.md' '\!\[.*?\]\(http[s]?:\/\/[^\)]*' . > tmp/tmp-http-md-1.txt
        
        # Printing all the HTTP/HTTPS images crawled in all the files.
        printf "\n-------------------------------------\nDETECTING HTTP/HTTPS IMAGES:\n-------------------------------------\n"
        cat tmp/tmp-http-md-1.txt
        printf -- "-------------------------------------\n\n"

        # Removing the textual part so that subsequent greps won't get false positive with "uploads/" in textual part, eg. [foo http://bar](http://foobar/...)
        grep -oh '\](http[s]\?://.*' tmp/tmp-http-md-1.txt > tmp/tmp-http-md-2.txt
        
        # Extracting the files to download
        grep -oh 'http[s]\?://.*[^\)]*' tmp/tmp-http-md-2.txt > tmp/http-resources-md-files-to-download.txt
        
        # Extracting the folders to create
        grep -oh 'http[s]\?://.*[\\\/]' tmp/http-resources-md-files-to-download.txt > tmp/http-resources-md-folders-to-create.txt
        
        # Renaming all the URLS except the downloading ones into real folders paths
        
        find . -name '*.md' -type f -print0 | xargs -0 sed -E -i 's/http:\//http/g'
        sed -i -E 's;http:/;http;g'  tmp/http-resources-md-folders-to-create.txt
        find . -name '*.md' -type f -print0 | xargs -0 sed -E -i 's/https:\//http/g'
        sed -i -E 's;https:/;http;g' tmp/http-resources-md-folders-to-create.txt

        # Creating /http subfolders
        printf "\n-------------------------------------\nCREATING FOLDERS STRUCTURE:\n-------------------------------------\n"
        while IFS='' read -r line || [[ -n "$line" ]]; do
          if [ ! -d ${line} ]; then
            mkdir -p ${line}
            printf "NEW FOLDER ========> Folder ${line} has been created\n"
          else
            printf "DUPLICATE FOLDER ==> Folder ${line} exists, safely skipping it...\n"
          fi
        done < "tmp/http-resources-md-folders-to-create.txt"
        printf -- "-------------------------------------\n\n"

        # Filling those subfolders with the resources
        printf "\n-------------------------------------\nDOWNLOADING HTTP/HTTPS IMAGES:\n-------------------------------------\n"
        while IFS='' read -r line || [[ -n "$line" ]]; do
          echo ${line}
          declare localPath
          localPath=$(sed 's/http[s]\?:\//http/' <<< ${line})
          if [ ! -f ${localPath} ]; then
            wget -O ${localPath} ${line}
          else
            printf "DUPLICATE FILE ====> File ${localPath} exists, safely skipping it...\n\n"
          fi 
        done < "tmp/http-resources-md-files-to-download.txt"
        printf -- "-------------------------------------\n\n"
      fi

    ########################################
    #     WIKI IMAGES - ASCIIDOC FILES     #
    ########################################
    - |
      if [ "$(grep -Roh --include '*.asciidoc' 'image:\{1,2\}/uploads/[^(\[|$|[:space:])]*' .)" == "" ]; then 
        echo "No Wiki resources to download"
      else
        
        grep -Roh --include '*.asciidoc' 'image:\{1,2\}/uploads/[^(\[|$|[:space:])]*' . > tmp/tmp-wiki-adoc-1.txt
        
        # Printing all the Wiki images crawled in all the files.        
        printf "\n-------------------------------------\nDETECTING WIKI IMAGES:\n-------------------------------------\n"
        cat tmp/tmp-wiki-adoc-1.txt
        # Using -- is necessary to explicit that options are over (since the row is starting with "--... and gets interpreted as an unrecognized option)
        printf -- "-------------------------------------\n\n"

        
        grep -oh ':\{1,2\}/uploads/.*' tmp/tmp-wiki-adoc-1.txt > tmp/tmp-wiki-adoc-2.txt
        # Extracting the files to download
        grep -oh 'uploads\/[^(\[|$|[:space:])]*' tmp/tmp-wiki-adoc-2.txt > tmp/wiki-resources-adoc-files-to-download.txt
        # Extracting the folders to create
        grep -oh 'upload.*[\\\/]' tmp/wiki-resources-adoc-files-to-download.txt > tmp/wiki-resources-adoc-folders-to-create.txt

        # removing leading slash to make url relative tu current dir
        find . -name '*.asciidoc' -type f -print0 | xargs -0 sed -E -i 's/\/uploads\//uploads\//g'  

        # Creating /uploads subfolders
        printf "\n-------------------------------------\nCREATING FOLDERS STRUCTURE:\n-------------------------------------\n"
        while IFS='' read -r line || [[ -n "$line" ]]; do
          if [ ! -d ${line} ]; then
            mkdir -p ../wiki/source/${line}
            printf "NEW FOLDER ========> Folder ${line} has been created\n"
          else
            printf "DUPLICATE FOLDER ==> Folder ${line} exists, safely skipping it...\n"
          fi
        done < "tmp/wiki-resources-adoc-folders-to-create.txt"
        printf -- "-------------------------------------\n\n"

        # Filling those subfolders with the resources
        printf "\n-------------------------------------\nDOWNLOADING WIKI IMAGES:\n-------------------------------------\n"
        while IFS='' read -r line || [[ -n "$line" ]]; do
          if [ ! -f ${line} ]; then
            wget -O ../wiki/source/${line} ${GITLAB_SCHEME}://${GITLAB_DOMAIN}/${CI_PROJECT_NAMESPACE}/${CI_PROJECT_NAME}/${line}
          else
            printf "DUPLICATE FILE ====> File ${line} exists, safely skipping it...\n\n"
          fi 
        done < "tmp/wiki-resources-adoc-files-to-download.txt"
      fi


    ########################################
    #  HTTP/HTTPS IMAGES - ASCIIDOC FILES  #
    ########################################
    - |
      if [ "$(grep -Roh --include '*.asciidoc' 'image:\{1,2\}http[s]\?://[^(\[|$|[:space:])]*' .)" == "" ]; then 
        echo "No HTTP/HTTPS resources to download"
      else

        # Matching only markdown images "![]()" (not markdown URLs like "[]()")
        grep -Roh --include '*.asciidoc' 'image:\{1,2\}http[s]\?://[^(\[|$|[:space:])]*' . > tmp/tmp-http-adoc-1.txt
        
        # Printing all the HTTP/HTTPS images crawled in all the files.
        printf "\n-------------------------------------\nDETECTING HTTP/HTTPS IMAGES:\n-------------------------------------\n"
        cat tmp/tmp-http-adoc-1.txt
        printf -- "-------------------------------------\n\n"

        
        grep -oh ':\{1,2\}http[s]\?://.*' tmp/tmp-http-adoc-1.txt > tmp/tmp-http-adoc-2.txt
        
        # Extracting the files to download
        grep -oh 'http[s]\?://.*[^(\[|$|[:space:])]*' tmp/tmp-http-adoc-2.txt > tmp/http-resources-adoc-files-to-download.txt
        
        # Extracting the folders to create
        grep -oh 'http[s]\?://.*[\\\/]' tmp/http-resources-adoc-files-to-download.txt > tmp/http-resources-adoc-folders-to-create.txt
        
        # Renaming all the URLS except the downloading ones into real folders paths
        find . -name '*.asciidoc' -type f -print0 | xargs -0 sed -E -i 's/http:\//http/g'
        sed -i 's/http:\//http/g'  tmp/http-resources-adoc-folders-to-create.txt
        find . -name '*.asciidoc' -type f -print0 | xargs -0 sed -E -i 's/https:\//http/g'
        sed -i 's/https:\//http/g' tmp/http-resources-adoc-folders-to-create.txt
      
        # Creating /http subfolders
        printf "\n-------------------------------------\nCREATING FOLDERS STRUCTURE:\n-------------------------------------\n"
        while IFS='' read -r line || [[ -n "$line" ]]; do
          if [ ! -d ${line} ]; then
            mkdir -p ${line}
            printf "NEW FOLDER ========> Folder ${line} has been created\n"
          else
            printf "DUPLICATE FOLDER ==> Folder ${line} exists, safely skipping it...\n"
          fi
        done < "tmp/http-resources-adoc-folders-to-create.txt"
        printf -- "-------------------------------------\n\n"

        # Filling those subfolders with the resources
        printf "\n-------------------------------------\nDOWNLOADING HTTP/HTTPS IMAGES:\n-------------------------------------\n"
        while IFS='' read -r line || [[ -n "$line" ]]; do
          echo ${line}
          declare localPath
          localPath=$(sed 's/http[s]\?:\//http/' <<< ${line})
          if [ ! -f ${localPath} ]; then
            wget -O ${localPath} ${line}
          else
            printf "DUPLICATE FILE ====> File ${localPath} exists, safely skipping it...\n\n"
          fi 
        done < "tmp/http-resources-adoc-files-to-download.txt"
        printf -- "-------------------------------------\n\n"
      fi

    # Let's create and fill WiKi folder so it'll appear as an untracked (not yet committed) folder, 
    # and move the files there so that they'll be available as artifacts to download
    - tar cf - * | (cd ../wiki/source; tar xf -)
    - ls -l
    - ls -l ../wiki/source
    - cd ..
    - rm -rf ${CI_PROJECT_NAME}.wiki
    - cd wiki/source
    
    # Move nested MarkDown and Asciidoc documents to the root, renaming them with the path
    - |
      find */ -name '*.md' | while read file; do
        mv "$file" "$(tr / ${SEPARATOR} <<< "$file")"
      done
    - |
      find */ -name '*.asciidoc' | while read file; do
        mv "$file" "$(tr / ${SEPARATOR} <<< "$file")"
      done

    # Let's start MarkDown to PDF conversion
    - printf "\n-------------------------------------\nSTARTING MARKDOWN CONVERSION:\n-------------------------------------\n"
    - find . -name '*.md' -exec sh -c 'echo "Processing $0" && kramdown $0 --output pdf > "${0%.*}".pdf && echo "Done.\n"' {} \;

    # Let's start AsciiDoc to PDF conversion
    - printf "\n-------------------------------------\nSTARTING ASCIIDOC CONVERSION:\n-------------------------------------\n"
    - find . -name '*.asciidoc' -exec sh -c 'echo "Processing $0" && asciidoctor-pdf -a icons=font $0 && echo "Done.\n"' {} \;

    # Cleanup
    - cd ../..
    - find ./wiki/source -name "*.pdf" -type f -exec mv {} ./wiki \;
    
    
  before_script:
    - | 
      printf -- "\n\n\n"
      printf -- " ___________________________________________________\n"
      printf -- "/                                                   \\ \n"
      printf -- "|                  Wiki2PDF v${SCRIPT_VERSION}                  |\n"
      printf -- "|                                                   |\n"
      printf -- "#===================================================#\n"
      printf -- "|                                                   |\n"
      printf -- "|         Turns GitLab's Wiki pages into PDF        |\n"
      printf -- "|                                                   |\n"
      printf -- "|                                                   |\n"
      printf -- "|                                                   |\n"
      printf -- "|             Created by Andrea Ligios              |\n"
      printf -- "|            Released under MIT License             |\n"
      printf -- "|                                                   |\n"
      printf -- "|    https://gitlab.com/Andrea.Ligios/Wiki2PDF/     |\n"
      printf -- "\\___________________________________________________/\n\n\n\n"

